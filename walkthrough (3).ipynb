{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Guided ML Walkthrough (Codespaces-ready)\n", "\n", "> Created/updated: ", "2025-09-16 07:56 ", "\n", "\n", "This notebook walks you through **supervised regression**, **supervised classification**, **unsupervised clustering**, **model validation**, and **saving models** using scikit-learn. It\u2019s designed for GitHub Codespaces and uses only widely available libraries.\n", "\n", "**How to use this notebook**\n", "1. Run the setup in the next section if this is a fresh Codespace.\n", "2. Execute cells from top to bottom. **Read the explanations** above each code block\u2014they tell you exactly what's happening and why.\n", "3. Inspect printed outputs, plots and tables; answer the checkpoint questions to test your understanding.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) One-time setup (Codespaces)\n", "**What & Why:** We create and activate a Python virtual environment so our dependencies don\u2019t clash with system Python. Then we install the exact libraries used in this notebook. Restarting the kernel lets Jupyter see the new packages.\n", "\n", "Open a terminal in your Codespace and run:\n", "```bash\n", "python -m venv .venv\n", "source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n", "pip install --upgrade pip\n", "pip install -r requirements.txt\n", "```\n", "Then **restart the kernel** (Kernel \u2192 Restart) so the packages are available to this notebook."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Warm-up: verify the tools\n", "**What this does:** Imports the core libraries and prints their versions to confirm that everything installed correctly. The `%matplotlib inline` directive ensures that plots render inside the notebook.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import core libs used throughout the notebook.\n", "import numpy as np, pandas as pd\n", "import matplotlib.pyplot as plt\n", "from sklearn import datasets, model_selection, metrics, preprocessing\n", "\n", "# Print versions so you can record your environment (useful for debugging/reproducibility).\n", "print(\"NumPy\", np.__version__)\n", "print(\"Pandas\", pd.__version__)\n", "print(\"Matplotlib\", plt.matplotlib.__version__)\n", "print(\"scikit-learn imported OK\")\n", "\n", "# Make plots appear inline in the notebook UI\n", "%matplotlib inline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Regression \u2014 predict a health score (Diabetes dataset)\n", "**Goal:** Predict a **continuous** target (disease progression) from 10 numeric features.\n", "\n", "**Why this approach?**\n", "- The dataset is built into scikit-learn \u2192 no downloads.\n", "- Linear Regression is the most basic regression model \u2192 great for introducing concepts like **train/test split**, **fit**, **predict**, and **evaluation** with MAE and R\u00b2.\n", "\n", "**What the code below does (step-by-step):**\n", "1. **Load** the dataset as a Pandas DataFrame (`as_frame=True`).\n", "2. **Split** into training and testing sets to simulate unseen data (20% held out). We fix `random_state` for reproducibility.\n", "3. **Fit** a `LinearRegression()` model on the training set (learns coefficients using least squares).\n", "4. **Predict** on the test set and compute **MAE** (average absolute error) and **R\u00b2** (explained variance).\n", "5. **Plot** the relationship between actual vs predicted to visually inspect systematic errors or scatter.\n", "6. **Inspect coefficients** to get a sense of which features push predictions up/down (interpretation caution: coefficients assume features are on comparable scales).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_diabetes\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_absolute_error, r2_score\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "# 1) Load data as (X, y). X has 10 features; y is a continuous target.\n", "data = load_diabetes(as_frame=True)\n", "X = data.data          # features (DataFrame)\n", "y = data.target        # target (Series)\n", "\n", "# 2) Split: we train on 80% and evaluate on 20% to measure generalisation performance.\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.2, random_state=0\n", ")\n", "\n", "# 3) Define and fit the model: finds coefficients that minimise squared errors on training data.\n", "linreg = LinearRegression()\n", "linreg.fit(X_train, y_train)\n", "\n", "# 4) Evaluate on held-out test set (data never seen during training).\n", "preds = linreg.predict(X_test)\n", "mae = mean_absolute_error(y_test, preds)        # average absolute difference\n", "r2  = r2_score(y_test, preds)                   # proportion of variance explained (1.0 is perfect)\n", "print(f\"MAE: {mae:.2f}, R\u00b2: {r2:.3f}\")\n", "\n", "# 5) Visual diagnostic: perfect predictions would lie exactly on the diagonal line.\n", "plt.figure()\n", "plt.scatter(y_test, preds)\n", "plt.xlabel(\"Actual target (y_test)\")\n", "plt.ylabel(\"Predicted target (\u0177)\")\n", "plt.title(\"Linear Regression: Actual vs Predicted\")\n", "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])\n", "plt.show()\n", "\n", "# 6) Coefficient table: larger magnitude = stronger linear effect on predictions (sign indicates direction).\n", "coef_table = pd.DataFrame({\n", "    \"feature\": X.columns,\n", "    \"coefficient\": linreg.coef_\n", "}).sort_values(\"coefficient\", ascending=False)\n", "coef_table"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Checkpoint (think & write):**\n", "- What do **MAE** and **R\u00b2** each reveal about model performance?\n", "- Which features have the strongest positive/negative coefficients? Does that make intuitive sense?\n", "- If features are on different scales, how might scaling change coefficient comparisons?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Classification \u2014 Iris dataset (KNN vs Logistic Regression)\n", "**Goal:** Predict a **categorical** target: one of three iris species.\n", "\n", "**Why two models?**\n", "- **KNN** is a simple, distance-based method; scaling helps because distances are measured feature-by-feature.\n", "- **Logistic Regression** learns a linear decision boundary in feature space and outputs class probabilities.\n", "\n", "**What the code below does:**\n", "1. **Load** the iris dataset (150 rows, 4 features, 3 classes).\n", "2. **Stratified split** so class ratios are similar in train/test.\n", "3. Build **pipelines** that first **scale** features, then apply the model. Pipelines help prevent **data leakage** and keep preprocessing + model together.\n", "4. **Fit** both models and **compare** accuracies.\n", "5. Print a **classification report** (precision/recall/F1 for each class) and a **confusion matrix** for error analysis.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_iris\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n", "import matplotlib.pyplot as plt\n", "\n", "# 1) Load data\n", "iris = load_iris(as_frame=True)\n", "X = iris.data\n", "y = iris.target\n", "\n", "# 2) Stratified split preserves class proportions in train/test.\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, stratify=y, test_size=0.2, random_state=0\n", ")\n", "\n", "# 3) KNN pipeline: scaling is critical for distance-based KNN.\n", "knn = Pipeline(steps=[\n", "    (\"scaler\", StandardScaler()),\n", "    (\"model\", KNeighborsClassifier(n_neighbors=5))\n", "])\n", "knn.fit(X_train, y_train)\n", "knn_preds = knn.predict(X_test)\n", "\n", "# 4) Logistic Regression pipeline: scaling generally helps convergence/stability.\n", "logreg = Pipeline(steps=[\n", "    (\"scaler\", StandardScaler()),\n", "    (\"model\", LogisticRegression(max_iter=200))\n", "])\n", "logreg.fit(X_train, y_train)\n", "log_preds = logreg.predict(X_test)\n", "\n", "# 5) Compare accuracies and inspect detailed metrics.\n", "print(\"KNN accuracy:\", accuracy_score(y_test, knn_preds))\n", "print(\"LogReg accuracy:\", accuracy_score(y_test, log_preds))\n", "\n", "print(\"\\nLogReg classification report (per-class precision/recall/F1):\")\n", "print(classification_report(y_test, log_preds, target_names=iris.target_names))\n", "\n", "# Confusion matrix: rows = actual, cols = predicted\n", "cm = confusion_matrix(y_test, log_preds)\n", "fig, ax = plt.subplots()\n", "im = ax.imshow(cm)\n", "ax.set_xticks(range(3)); ax.set_yticks(range(3))\n", "ax.set_xticklabels(iris.target_names); ax.set_yticklabels(iris.target_names)\n", "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.title(\"Confusion Matrix (LogReg)\")\n", "for i in range(3):\n", "    for j in range(3):\n", "        ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n", "plt.colorbar(im)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Checkpoint:**\n", "- Which model scored higher **accuracy** on your split? Why might that be?\n", "- Using the confusion matrix, which classes get confused? What features might separate them better?\n", "- How would performance change if we **didn\u2019t** scale features for KNN?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Unsupervised \u2014 K-Means clustering on Iris\n", "**Goal:** Discover structure in data **without labels** and then compare clusters to the true species (for learning purposes only\u2014the model has no access to labels).\n", "\n", "**Key ideas:**\n", "- K-Means tries to partition points into `k` clusters by minimising within-cluster variance.\n", "- Results can depend on initialisation; we use `n_init=10` and a fixed random seed for reproducibility.\n", "- Clusters won\u2019t necessarily match the true classes\u2014unsupervised learning optimises a different objective.\n", "\n", "**What the code does:**\n", "1. Fit K-Means with `k=3` (since Iris has 3 species) and get cluster assignments.\n", "2. Use **PCA** to reduce the 4D features to 2D for visualisation only (PCA is unsupervised dimensionality reduction).\n", "3. Cross-tab cluster IDs vs true labels to see alignment.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.cluster import KMeans\n", "from sklearn.decomposition import PCA\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "# 1) Fit KMeans with 3 clusters (we know iris has 3 species).\n", "kmeans = KMeans(n_clusters=3, n_init=10, random_state=0)\n", "clusters = kmeans.fit_predict(X)\n", "\n", "# 2) Visualise in 2D using PCA (for display only; the model is trained on the original 4D space).\n", "pca = PCA(n_components=2, random_state=0)\n", "X_2d = pca.fit_transform(X)\n", "\n", "plt.figure()\n", "plt.scatter(X_2d[:,0], X_2d[:,1], c=clusters)\n", "plt.title(\"K-Means clusters on Iris (PCA 2D)\")\n", "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n", "plt.show()\n", "\n", "# 3) Compare clusters with true labels (for evaluation/learning only).\n", "mapping_table = pd.crosstab(pd.Series(clusters, name=\"cluster\"),\n", "                             pd.Series(y, name=\"true_label\"))\n", "mapping_table.index.name = \"kmeans_cluster\"\n", "mapping_table.columns = iris.target_names\n", "mapping_table"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Checkpoint:**\n", "- Do the clusters align with the species reasonably well? Which species is hardest to recover via K-Means?\n", "- Try different `n_clusters` (e.g., 2 or 4). How does the cross-tab change? Why?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Model validation \u2014 choose K via cross-validation\n", "**Goal:** Use **K-fold cross-validation (CV)** on the **training set only** to select the best hyperparameter `k` for KNN **without peeking at the test set**. This helps prevent optimistic bias.\n", "\n", "**What the code does:**\n", "1. For `k` from 1 to 20, build a pipeline (**scaler + KNN**) and compute **5-fold CV accuracy** on the training data.\n", "2. Choose the `k` with the highest mean CV accuracy.\n", "3. Refit on the full training set using the best `k` and evaluate once on the untouched test set.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import cross_val_score\n", "import numpy as np\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.metrics import accuracy_score\n", "import matplotlib.pyplot as plt\n", "\n", "ks = range(1, 21)\n", "cv_scores = []\n", "for k in ks:\n", "    # IMPORTANT: The scaler is inside the CV loop via Pipeline \u2192 avoids data leakage.\n", "    pipe = Pipeline(steps=[\n", "        (\"scaler\", StandardScaler()),\n", "        (\"model\", KNeighborsClassifier(n_neighbors=k))\n", "    ])\n", "    # 5-fold CV on the training set only\n", "    scores = cross_val_score(pipe, X_train, y_train, cv=5)\n", "    cv_scores.append(scores.mean())\n", "\n", "best_k = ks[int(np.argmax(cv_scores))]\n", "print(\"Best k by CV:\", best_k)\n", "\n", "# Visualise the CV curve to see how k affects bias/variance.\n", "plt.figure()\n", "plt.plot(list(ks), cv_scores, marker=\"o\")\n", "plt.xlabel(\"k (neighbors)\")\n", "plt.ylabel(\"Mean CV accuracy (5-fold, train set)\")\n", "plt.title(\"KNN: choose k via cross-validation\")\n", "plt.show()\n", "\n", "# Final evaluation on the untouched test set using the selected k.\n", "best_knn = Pipeline(steps=[\n", "    (\"scaler\", StandardScaler()),\n", "    (\"model\", KNeighborsClassifier(n_neighbors=best_k))\n", "]).fit(X_train, y_train)\n", "print(\"Test accuracy with best k:\", accuracy_score(y_test, best_knn.predict(X_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Checkpoint:**\n", "- Does the CV-selected `k` improve test accuracy compared to `k=5`? If not, why might that be?\n", "- How would using the **test set** to pick `k` lead to overfitting?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) End-to-end pipeline with SVM\n", "**Goal:** Demonstrate a common production-style workflow: combine preprocessing and model in a single object that can be fit, evaluated, and saved.\n", "\n", "**Key parameters:**\n", "- `kernel=\"rbf\"` allows non-linear decision boundaries.\n", "- `C` controls regularisation strength (higher C = fit training data more tightly).\n", "- `gamma=\"scale\"` sets a sensible default for the RBF kernel.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.svm import SVC\n", "from sklearn.metrics import accuracy_score\n", "\n", "# Build a single pipeline that encapsulates preprocessing + model.\n", "svm_clf = Pipeline(steps=[\n", "    (\"scaler\", StandardScaler()),\n", "    (\"model\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", probability=True))\n", "])\n", "\n", "# Fit on training data and evaluate on test data.\n", "svm_clf.fit(X_train, y_train)\n", "print(\"SVM test accuracy:\", accuracy_score(y_test, svm_clf.predict(X_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Save & load a trained model (deployment starter)\n", "**Why save models?** To reuse a trained pipeline in another script or service (e.g., a CLI tool or a small web app) **without retraining**.\n", "\n", "**What the code does:**\n", "1. Save the trained SVM pipeline to a file with `joblib.dump`.\n", "2. Load it back with `joblib.load`.\n", "3. Verify the loaded model produces the same predictions/accuracy.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import joblib\n", "from sklearn.metrics import accuracy_score\n", "\n", "# 1) Save to disk\n", "joblib.dump(svm_clf, \"iris_svm_pipeline.joblib\")\n", "print(\"Saved to iris_svm_pipeline.joblib\")\n", "\n", "# 2) Load from disk\n", "loaded = joblib.load(\"iris_svm_pipeline.joblib\")\n", "\n", "# 3) Sanity check: accuracy should match\n", "print(\"Reloaded accuracy:\", accuracy_score(y_test, loaded.predict(X_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Stretch goals (choose any)\n", "- Use `DecisionTreeClassifier` and inspect `feature_importances_` to see which features drive splits.\n", "- Compare models with and without `StandardScaler` to observe the impact on distance-based models.\n", "- For regression, try `Ridge`, `Lasso`, or `RandomForestRegressor` and compare MAE/R\u00b2.\n", "- Plot pairwise feature scatter using `plt.scatter` to see class separation.\n", "- Build a tiny CLI that loads `iris_svm_pipeline.joblib`, accepts four inputs, and prints the predicted species.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Troubleshooting\n", "- If plots don't show, run `%matplotlib inline` in a cell.\n", "- If a package import fails, re-run the setup commands in a terminal and **restart the kernel**.\n", "- If accuracy varies, remember that train/test splits are random unless you fix `random_state` (we did).\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}